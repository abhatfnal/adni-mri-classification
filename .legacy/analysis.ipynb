{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6517d178",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from data.datasets import ADNIDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.eval import load_model, create_slice_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "698c739c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_grad_cam import GradCAM, EigenCAM, GradCAMPlusPlus, LayerCAM,ScoreCAM, ShapleyCAM\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "from IPython.display import HTML\n",
    "import plotly.io as pio\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a296c6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradcam(model, sample, target_layer_name, target_class=1):\n",
    "    \n",
    "    model.eval()\n",
    "    out = model(sample)\n",
    "    \n",
    "    target_layer = None\n",
    "    for name, module in model.named_modules():\n",
    "        if name == target_layer_name:\n",
    "            target_layer = module\n",
    "            break\n",
    "    \n",
    "    with ShapleyCAM(model=model, target_layers=[ target_layer ]) as cam:\n",
    "       gcam_overlay = cam(sample, targets=[ ClassifierOutputTarget(target_class)])\n",
    "       \n",
    "    return gcam_overlay.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a757fd7",
   "metadata": {},
   "source": [
    "#### Analyze "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c8a2cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/project/aereditato/abhat/adni-mri-classification/experiments/mresnet18_20250821_190324_CN_vs_AD/'\n",
    "fold = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98db14f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "/project/aereditato/abhat/adni-mri-classification/experiments/mresnet18_20250821_190324_CN_vs_AD/model.py",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model, cfg \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/project/aereditato/cestari/adni-mri-classification/utils/eval.py:83\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(experiment_dir, fold, project_root, base_model_import, map_location, strict)\u001b[0m\n\u001b[1;32m     80\u001b[0m weights_path \u001b[38;5;241m=\u001b[39m experiment_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfold_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_model.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_py_path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[0;32m---> 83\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(model_py_path)\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m config_path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(config_path)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: /project/aereditato/abhat/adni-mri-classification/experiments/mresnet18_20250821_190324_CN_vs_AD/model.py"
     ]
    }
   ],
   "source": [
    "model, cfg = load_model(path, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "daf951cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ADNIDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load test dataset\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mADNIDataset\u001b[49m(cfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_csv\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      5\u001b[0m loader \u001b[38;5;241m=\u001b[39m DataLoader(test_dataset,\u001b[38;5;241m1\u001b[39m,shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ADNIDataset' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Load test dataset\n",
    "test_dataset = ADNIDataset(cfg['data']['test_csv'])\n",
    "loader = DataLoader(test_dataset,1,shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "samples = []\n",
    "labels = []\n",
    "preds = []\n",
    "for imgs,lbls in loader:\n",
    "    samples.append(imgs)\n",
    "    labels.append(lbls)\n",
    "    pred = torch.round(torch.sigmoid(model(imgs))).squeeze(1)\n",
    "    preds.append(pred.detach().numpy())\n",
    "\n",
    "report = pd.DataFrame(classification_report(labels, preds, output_dict=True)).T\n",
    "cm = pd.DataFrame(confusion_matrix(labels, preds))\n",
    "\n",
    "print(report)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eafaaf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature map shape: torch.Size([16, 19, 23, 19])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only integer tensors of a single element can be converted to an index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m vis \u001b[38;5;241m=\u001b[39m pred\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fmap\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m vis \u001b[38;5;241m<\u001b[39m fmap\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 25\u001b[0m     sel \u001b[38;5;241m=\u001b[39m \u001b[43mfmap\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvis\u001b[49m\u001b[43m:\u001b[49m\u001b[43mvis\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m   \u001b[38;5;66;03m# (1,1,d',h',w')\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     27\u001b[0m     sel \u001b[38;5;241m=\u001b[39m fmap[:, :\u001b[38;5;241m1\u001b[39m]           \u001b[38;5;66;03m# fallback to first channel\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: only integer tensors of a single element can be converted to an index"
     ]
    }
   ],
   "source": [
    "# ...existing code...\n",
    "idx = 35\n",
    "\n",
    "model.eval()\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model(samples[idx])                     # (1, num_classes)\n",
    "    pred = torch.round(torch.sigmoid(out)).squeeze(1)   # predicted class as int\n",
    "\n",
    "    fmap = model.feature_maps.squeeze() # get feature map from model                   \n",
    "\n",
    "    print(f\"Feature map shape: {fmap.shape}\")\n",
    "    \n",
    "    # make sure fmap has a batch dim: (1, C, d, h, w) or (1, 1, d, h, w)\n",
    "    if fmap.dim() == 4:       # (C, d, h, w) or (d, h, w)\n",
    "        fmap = fmap.unsqueeze(0)\n",
    "    if fmap.dim() == 3:       # (d, h, w)\n",
    "        fmap = fmap.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    # select channel corresponding to predicted class when possible\n",
    "    vis = pred\n",
    "    if fmap.size(1) > 1 and vis < fmap.size(1):\n",
    "        sel = fmap[:, vis:vis+1]   # (1,1,d',h',w')\n",
    "    else:\n",
    "        sel = fmap[:, :1]           # fallback to first channel\n",
    "\n",
    "    target_size = samples[idx].shape[-3:]   # (D,H,W) e.g. (79,95,79)\n",
    "    up = F.interpolate(sel.float(), size=target_size, mode='trilinear', align_corners=False)\n",
    "    feature_map_upsampled = up.squeeze().cpu()   # (D,H,W)\n",
    "\n",
    "print(f\"True label: {labels[idx][0].item()}, Predicted: {pred}\")\n",
    "\n",
    "gradcam_vol = feature_map_upsampled\n",
    "figs = [\n",
    "    create_slice_plot(samples[idx], axis=0, gradcam_volume=gradcam_vol),\n",
    "    create_slice_plot(samples[idx], axis=1, gradcam_volume=gradcam_vol),\n",
    "    create_slice_plot(samples[idx], axis=2, gradcam_volume=gradcam_vol),\n",
    "]\n",
    "\n",
    "html_figs = [pio.to_html(fig, full_html=False, include_plotlyjs='cdn') for fig in figs]\n",
    "HTML(f\"<div style='display:flex;gap:0px;'>{''.join(html_figs)}</div>\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e5cc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 94\n",
    "\n",
    "print(f\"True label: {labels[idx]}\")\n",
    "print(f\"Predicted label: {pred[idx]}\")\n",
    "\n",
    "gradcam_vol = compute_gradcam(model, samples[idx], 'b4', pred[idx])\n",
    "figs = [\n",
    "    create_slice_plot(samples[idx], axis=0, gradcam_volume=gradcam_vol),\n",
    "    create_slice_plot(samples[idx], axis=1, gradcam_volume=gradcam_vol),\n",
    "    create_slice_plot(samples[idx], axis=2, gradcam_volume=gradcam_vol),\n",
    "]\n",
    "\n",
    "html_figs = [pio.to_html(fig, full_html=False, include_plotlyjs='cdn') for fig in figs]\n",
    "HTML(f\"<div style='display:flex;gap:0px;'>{''.join(html_figs)}</div>\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1838afa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize average Grad-CAM\n",
    "avg_cn_cam = 0\n",
    "avg_mci_cam = 0\n",
    "avg_ad_cam = 0\n",
    "\n",
    "# Initialize average volumes\n",
    "avg_cn_vol = 0\n",
    "avg_mci_vol = 0\n",
    "avg_ad_vol = 0\n",
    "\n",
    "# Predictions\n",
    "pred = []\n",
    "\n",
    "# Compute Grad-CAM for each sample and save to file\n",
    "for i, sample in enumerate(samples):\n",
    "    \n",
    "    print(\"Processing sample\", i+1, \"/\", len(samples), end='\\r')\n",
    "    \n",
    "    cam_overlay = compute_gradcam(model, sample, 'b4', labels[i])\n",
    "    \n",
    "    # compile prediction\n",
    "    pred.append(model(sample).max(1, keepdim=True)[1].detach().numpy())\n",
    "    \n",
    "    if labels[i] == 0:\n",
    "        avg_cn_cam += cam_overlay\n",
    "        avg_cn_vol += samples[i]\n",
    "    elif labels[i] == 1:\n",
    "        avg_mci_cam += cam_overlay\n",
    "        avg_mci_vol += samples[i]\n",
    "    elif labels[i] == 2:\n",
    "        avg_ad_cam += cam_overlay\n",
    "        avg_ad_vol += samples[i]\n",
    "    \n",
    "# Labels counts\n",
    "num_cn = sum([1 for lbl in labels if lbl == 0])\n",
    "num_mci = sum([1 for lbl in labels if lbl == 1])\n",
    "num_ad = sum([1 for lbl in labels if lbl == 2])\n",
    "\n",
    "# Average Grad-CAMs\n",
    "if num_cn > 0:\n",
    "    avg_cn_cam /= num_cn\n",
    "    avg_cn_vol /= num_cn\n",
    "    \n",
    "if num_mci > 0:\n",
    "    avg_mci_cam /= num_mci\n",
    "    avg_mci_vol /= num_mci\n",
    "    \n",
    "if num_ad > 0:\n",
    "    avg_ad_cam /= num_ad\n",
    "    avg_ad_vol /= num_ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c176b992",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol = avg_ad_vol\n",
    "cam_vol = avg_ad_cam\n",
    "\n",
    "figs = [\n",
    "    create_slice_plot(vol, axis=0, gradcam_volume=cam_vol),\n",
    "    create_slice_plot(vol, axis=1, gradcam_volume=cam_vol),\n",
    "    create_slice_plot(vol, axis=2, gradcam_volume=cam_vol),\n",
    "]\n",
    "\n",
    "html_figs = [pio.to_html(fig, full_html=False, include_plotlyjs='cdn') for fig in figs]\n",
    "HTML(f\"<div style='display:flex;gap:0px;'>{''.join(html_figs)}</div>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a42f62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adni_rcc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
