# ADNI MRI Classification

Framework for training and evaluating deep learning models on the ADNI MRI dataset. This repository provides:

- **Flexible configuration** via YAML files (mergeable, override-able)
- **Easy job submission** to RCC or local runs
- **Automatic experiment logging** under `experiments/`
- **Modular model registry** for custom architectures
- **Preprocessing pipelines** with reusable scripts
- **Utility tools** for dataset management and visualization

---

## ğŸ“ Repository Structure

```
â”œâ”€â”€ configs
â”‚   â”œâ”€â”€ schema.py                # Config schema & validation
â”‚   â””â”€â”€ training                 # YAML configs for various models/training
â”‚       â”œâ”€â”€ custom_3dcnn.yaml
â”‚       â”œâ”€â”€ default.yaml
â”‚       â”œâ”€â”€ example.yaml
â”‚       â”œâ”€â”€ resnet18.yaml
â”‚       â””â”€â”€ simple_3dcnn.yaml
â”œâ”€â”€ data
â”‚   â”œâ”€â”€ augmentation.py
â”‚   â”œâ”€â”€ datasets.py
â”‚   â”œâ”€â”€ DXSUM_05Apr2025.csv      # ADNI subject metadata
â”‚   â”œâ”€â”€ preprocessing_dicom
â”‚   â”‚   â”œâ”€â”€ data/                # Preprocessed .nii files + CSV labels
â”‚   â”‚   â””â”€â”€ run.sh               # Runs preprocessing
â”‚   â”œâ”€â”€ preprocessing_dicom_spm  # Alternative SPM-based pipeline
â”‚   â””â”€â”€ ...
â”œâ”€â”€ experiments/                 # Auto-created subfolders on each run: logs, metrics, configs
â”œâ”€â”€ guide.md                     # (This document)
â”œâ”€â”€ models
â”‚   â”œâ”€â”€ base.py                  # `BaseModel` abstract class
â”‚   â”œâ”€â”€ model_custom_3dcnn.py
â”‚   â”œâ”€â”€ model_resnet18.py
â”‚   â”œâ”€â”€ model_simple_3dcnn.py
â”‚   â”œâ”€â”€ model_simple_3dcnn_gradcam.py
â”‚   â””â”€â”€ registry.py              # Maps model names â†’ classes
â”œâ”€â”€ train.py                     # CLI entrypoint
â””â”€â”€ utils
    â”œâ”€â”€ ...
    â””â”€â”€ train_test_split.py      # Split CSV into train/val & test sets
```

---

## ğŸ“¦ Requirements

- Python 3.10
- Matlab
- [SPM 12](https://www.fil.ion.ucl.ac.uk/spm/software/spm12/)
---

## ğŸš€ Quick Start

1. **Install requirements** (e.g. in a Conda env):

   ```bash
   pip install -r requirements.txt
   ```

2. **Prepare data**  
   Follow the instructions in `data/preprocessing_dicom/` (or the SPM variant) to generate your `.nii` files and labels CSV under `data/.../data/`.

3. **Split your dataset**:  
   ```bash
   python utils/train_test_split.py      --input-csv /path/to/full_dataset.csv      --output-dir /path/to/preprocessed/data
   ```

4. **Run training**:

   ```bash
   python train.py      --config configs/training/simple_3dcnn.yaml      --config configs/training/custom_3dcnn.yaml      --config configs/training/override.yaml      --job          # omit for local run
   ```

   - You can pass **multiple** `--config` files; they are merged in order.
   - To override a single field on the CLI:
     ```
     python train.py --config default.yaml training.epochs=500
     ```

---

## âš™ï¸ Configuration

All default settings live in `configs/training/default.yaml`. A full example:

```yaml
training:
  batch_size: 32
  epochs: 200
  optimizer:
    name: adam
    weight_decay: 0
    lr: 1e-3
  scheduler:
    name: CosineAnnealingLR
    t_max: 200
    lr_max: 1e-3
    lr_min: 2e-6

data:
  trainval_csv: /absolute/path/to/trainval.csv
  test_csv:     /absolute/path/to/test.csv

cross_validation:
  folds: 5
```

- **Merging & Overrides**  
  Later files override earlier ones. Command-line overrides (e.g. `training.epochs=500`) take highest precedence.

- **Schema validation** is enforced by `configs/schema.py`. Bad or missing fields will raise errors before any training begins.

---

## ğŸ“Š Experiment Logging

Each run creates a new folder under `experiments/`, named by timestamp or job ID. Inside youâ€™ll find:

- A copy of the **merged YAML** config
- `losses.vs` with per-epoch  train and validation losses
- **Metrics** CSV: accuracy, AUC, per-class recall/precision
- **Confusion matrices** for each fold + overall average
- Any model checkpoints (`.pth`)

This structure makes it easy to reproduce and compare runs.

---

## ğŸ§  Model Registry

- To add a new model:
  1. Implement a subclass of `models/base.py` (must accept a `dict` of params).
  2. Add it to `models/registry.py` under a unique key.
- At runtime, `train.py` looks up the YAMLâ€™s `model.name` in the registry and instantiates it.

---

## ğŸ—‚ï¸ Data & Preprocessing

- Each preprocessing folder (e.g. `preprocessing_dicom_spm/`) contains:
  - A `data/` subfolder with `.nii` files + `<split>.csv` (paths + labels)
  - Scripts or instructions to reproduce the preprocessing

---

## ğŸ› ï¸ Utilities

- **Dataset management**:  
  - `get_scan_list.py` â†’ list files in archives  
  - `train_test_split.py` â†’ generate train/val/test CSVs
- **Visualization**:  
  - `plot_training_log.py` â†’ loss & metric curves  
  - `gradcam_visualize.py` â†’ saliency maps  
  - `plot_histogram.py` â†’ data distribution

---

## ğŸ” Examples

**Local train with ResNet18 backbone:**

```bash
python train.py   --config configs/training/resnet18.yaml data.trainval_csv=./data/preprocessing_spm/data/trainval.csv   data.test_csv=./data/preprocessing_spm/data/test.csv 
```

**Submit to RCC, override epochs & LR:**

```bash
python train.py   --config configs/training/custom_3dcnn.yaml   --job   training.epochs=300   training.optimizer.lr=5e-4
```

---